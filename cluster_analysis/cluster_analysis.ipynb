{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing, cluster\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(data, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data[name] = le.fit_transform(data[name])\n",
    "    return le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = pd.read_csv(\"./imdb_dataset.csv\")\n",
    "# Seems the csv file is missing the 1st column name\n",
    "imdb_dataset.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "print(\"All columns: \", imdb_dataset.columns)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Partitioning\n",
    "Select features for K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = imdb_dataset[['title', 'genre', 'mpaa_rating', 'imdb_rating', 'critics_score', 'audience_rating','audience_score']]\n",
    "db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess categorical columns and normalize numerical columns\n",
    "Note: we drop 'title' as is not informative for K-means clustering and 'genre' because we want to use 'genre' later to analyze our clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2_preprocessed = pd.get_dummies(db2.drop(columns=['title', 'genre']), dtype=int)\n",
    "# Z-score each column\n",
    "db2_preprocessed = (db2_preprocessed-db2_preprocessed.mean()) / db2_preprocessed.std()\n",
    "db2_preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the optimal number of clusters for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All genres: \", db2['genre'].unique())\n",
    "print(\"Number of different movie genres in the dataset: \", len(imdb_dataset['genre'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot SSE vs # of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = range(1, 40)\n",
    "SSE = []\n",
    "for k in numClusters:\n",
    "    k_means = cluster.KMeans(n_clusters=k, n_init=10)\n",
    "    k_means.fit(db2_preprocessed)\n",
    "    SSE.append(k_means.inertia_)\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.plot(numClusters, SSE, marker='o', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing n_clusters=9 or n_clusters=16 looks like a good fit on the elbow line. We pick n_clusters=16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Cluster\n",
    "Split data and keep a small portion (10%) for analyzing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ind = int(len(db2_preprocessed) * 0.9)\n",
    "data_train = db2_preprocessed[:split_ind]\n",
    "data_test = db2_preprocessed[split_ind:]\n",
    "print(f\"Train samples: {len(data_train)}, Test samples: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = cluster.KMeans(n_clusters=n_clusters, max_iter=100, n_init=10, random_state=1)\n",
    "k_means.fit(data_train) \n",
    "labels = k_means.labels_\n",
    "print(\"Unique cluster ids: \", np.unique(labels))\n",
    "clusters_train_df = pd.DataFrame(labels, index=db2.title[:split_ind], columns=['Cluster ID'])\n",
    "clusters_train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append 'genre' column to analyze our clusters on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append 'genre' column to analyze our clusters\n",
    "clusters_train_df['genre'] = db2.genre[:split_ind].values\n",
    "clusters_train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze clusters for genre composition.\n",
    "Ideally clusters should show grouping of similar genres. </br>\n",
    "Our clusters have good genre composition as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genre composition for cluster 1\")\n",
    "print(clusters_train_df.groupby(['Cluster ID', 'genre']).size()[1])\n",
    "\n",
    "print(\"Genre composition for cluster 2\")\n",
    "print(clusters_train_df.groupby(['Cluster ID', 'genre']).size()[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to visualize our clusters in 2 dimensions\n",
    "We project the training data to 2D with PCA and then color each sample (movie) with the cluster id color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
    "\n",
    "data_train_2D = pd.DataFrame(KernelPCA(n_components=2, kernel='linear').fit_transform(data_train), columns=['PC1', 'PC2'])\n",
    "data_train_2D.plot.scatter(x='PC1', y='PC2', c=clusters_train_df['Cluster ID'], colormap='tab20c', ax = axes[0], subplots=True)\n",
    "\n",
    "color_labels = clusters_train_df['genre'].unique()\n",
    "rgb_values = sns.color_palette(\"Set2\", 11)\n",
    "color_map = dict(zip(color_labels, rgb_values))\n",
    "data_train_2D.plot.scatter(x='PC1', y='PC2', c=clusters_train_df['genre'].map(color_map), title='PCA projection of training data colored by cluster ID (left) and by genre (right)', ax = axes[1], subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "centroids = k_means.cluster_centers_\n",
    "centroids_df = pd.DataFrame(centroids,columns=data_train.columns)\n",
    "pd.DataFrame(KernelPCA(n_components=2, kernel='rbf').fit_transform(centroids_df), columns=['PC1', 'PC2']) \\\n",
    "    .plot.scatter(x='PC1', y='PC2', title=\"2D Visualization (PCA projection) of KMeans centroids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply trained KMeans algorithm to the held out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster labels for unseen movies using trained KMeans\n",
    "labels = k_means.predict(data_test)\n",
    "labels = labels.reshape(-1,1)\n",
    "# Print SSE on test\n",
    "print(\"Model inertia: \", k_means.inertia_)\n",
    "\n",
    "# Create a dataframe that has new movies and their cluster assignment\n",
    "newmovies = db2[split_ind:].copy()\n",
    "newmovies['Cluster ID'] = labels\n",
    "print(\"Cluster allocation for new, unused in training movies\")\n",
    "newmovies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Analysis on the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_text_index(imdb_dataset, 'title_type')\n",
    "encode_text_index(imdb_dataset, 'mpaa_rating')\n",
    "encode_text_index(imdb_dataset, 'critics_rating')\n",
    "encode_text_index(imdb_dataset, 'audience_rating')\n",
    "encode_text_index(imdb_dataset, 'best_pic_nom')\n",
    "encode_text_index(imdb_dataset, 'best_pic_win')\n",
    "encode_text_index(imdb_dataset, 'best_actor_win')\n",
    "encode_text_index(imdb_dataset, 'best_actress_win')\n",
    "encode_text_index(imdb_dataset, 'best_dir_win')\n",
    "encode_text_index(imdb_dataset, 'top200_box')\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the dataset so clustering plot is more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "Y = imdb_dataset['genre']\n",
    "X = imdb_dataset.drop(['id', 'title','genre', 'runtime', 'studio', 'thtr_rel_year', 'thtr_rel_month', 'thtr_rel_day', 'dvd_rel_year', 'dvd_rel_month', 'dvd_rel_day',\n",
    "               'director', 'actor1', 'actor2', 'actor3', 'actor4', 'actor5', 'imdb_url', 'rt_url'],axis=1)\n",
    "\n",
    "# Minimizing the rows by choosing 40 random movies\n",
    "names = imdb_dataset['title'].sample(n=40, random_state=0)\n",
    "X = X.sample(n=40, random_state=0)\n",
    "\n",
    "Z = hierarchy.linkage(X.values, 'single')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X.values, 'complete')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X.values, 'average')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for text mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = [ 'Now for manners use has company believe parlors.',\n",
    "'Least nor party who wrote while did. Excuse formed as is agreed admire so on result parish.',\n",
    "'Put use set uncommonly announcing and travelling. Allowance sweetness direction to as necessary.',\n",
    "'Principle oh explained excellent do my suspected conveying in.',\n",
    "'Excellent you did therefore perfectly supposing described. ',\n",
    "'Its had resolving otherwise she contented therefore.',\n",
    "'Afford relied warmth out sir hearts sister use garden.',\n",
    "'Men day warmth formed admire former simple.',\n",
    "'Humanity declared vicinity continue supplied no an. He hastened am no property exercise of. ' ,\n",
    "'Dissimilar comparison no terminated devonshire no literature on. Say most yet head room such just easy. ']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vector Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n",
    "vectorizer = sk_text.CountVectorizer(min_df=2)\n",
    "#vectorizer = sk_text.CountVectorizer(stop_words = 'english')\n",
    "\n",
    "#min_df: ignore terms that have a document frequency < min_df.\n",
    "\n",
    "matrix = vectorizer.fit_transform(text_dataset)\n",
    "\n",
    "print(type(matrix))          # Compressed Sparse Row matrix\n",
    "print(matrix.toarray())        #  convert it to numpy array\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vector Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sk_text.TfidfVectorizer(\n",
    "                             #stop_words='english',\n",
    "                             #max_features = 1000,\n",
    "                             min_df=2)\n",
    "\n",
    "\n",
    "#max_features:  build a vocabulary that only consider the top max_features features ordered by term frequency across the corpus.\n",
    "\n",
    "matrix = vectorizer.fit_transform(text_dataset)\n",
    "\n",
    "print(type(matrix))          # Compressed Sparse Row matrix\n",
    "print(matrix.toarray())        #  convert it to numpy array\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4) Tfidf (term frequency-inverse document frequency) is a measure of how frequent a word appears in a set of documents. It is generally used in text analysis algorithms and for document searching. For example, Google search uses Tfidf for text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def change_to_binary_values(df, col_name):\n",
    "    df[col_name] = (df[col_name] > df[col_name].median()).astype('int')\n",
    "    \n",
    "#Function to normalize columns\n",
    "def normalize_numeric_minmax(df, name):\n",
    "        df[name] = ((df[name] - df[name].min()) / (df[name].max() - df[name].min())).astype(np.float32)\n",
    "        \n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name], prefix=name)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "admission_dataset = pd.read_csv(\"./Admission_Predict_Ver1.1_small_data_set_for_Linear_Regression-1.csv\")\n",
    "admission_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_to_binary_values(admission_dataset, 'GRE Score')\n",
    "change_to_binary_values(admission_dataset, 'TOEFL Score')\n",
    "change_to_binary_values(admission_dataset, 'University Rating')\n",
    "change_to_binary_values(admission_dataset, 'SOP')\n",
    "change_to_binary_values(admission_dataset, 'LOR ')\n",
    "change_to_binary_values(admission_dataset, 'CGPA')\n",
    "admission_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "admission_dataset = encode_text_dummy(admission_dataset, 'University Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = admission_dataset.drop('Chance of Admit ', axis=1)\n",
    "y = admission_dataset['Chance of Admit ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
