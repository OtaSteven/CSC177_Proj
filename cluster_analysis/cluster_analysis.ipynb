{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation to install on python/anaconda <br />\n",
    "`pip install tensorflow` <br />\n",
    "`pip install seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block is made more as a setup with giving all the necessary imports and functions to use for Cluster Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing, cluster\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(data, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data[name] = le.fit_transform(data[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Create dummies columns from categorical values\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name], prefix=name)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take the imdb dataset and print it while fixing a small issue with the unnamed numbered column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = pd.read_csv(\"./imdb_dataset.csv\")\n",
    "# Seems the csv file is missing the 1st column name\n",
    "imdb_dataset.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "print(\"All columns: \", imdb_dataset.columns)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Partitioning\n",
    "Select features for K-means clustering. Because of the large size of data, we first need to partition it in order to prepare us for clustering analysis. We first do vertical partitioning in order to isolate the necessary columns we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = imdb_dataset[['title', 'genre', 'mpaa_rating', 'imdb_rating', 'critics_score', 'audience_rating','audience_score']]\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess categorical columns and normalize numerical columns\n",
    "Note: we drop 'title' as is not informative for K-means clustering and 'genre' because we want to use 'genre' later to analyze our clusters\n",
    "\n",
    "The next step for preprocessing we use is hot encoding in order to binarize the db2 values into 0s to 1s using the z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = encode_text_dummy(db, 'mpaa_rating')\n",
    "db2 = encode_text_dummy(db2, 'audience_rating')\n",
    "db2_preprocessed = db2.drop(columns=['title', 'genre'])\n",
    "# # Z-score each column\n",
    "db2_preprocessed = (db2_preprocessed-db2_preprocessed.mean()) / db2_preprocessed.std()\n",
    "db2_preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the optimal number of clusters for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All genres: \", db2['genre'].unique())\n",
    "print(\"Number of different movie genres in the dataset: \", len(imdb_dataset['genre'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot SSE vs # of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the necessary data preprocessed, we can now take it in order to apply k-means analysis with a number of clusters range from 1 to 40 for getting the amount of clusters compared to the Sum Squared Errors letting us see the distance of each data point from our range of clusters. \n",
    "\n",
    "With that data now, we apply it with the matplotlib in order to graph the elbow method of the SSE vs # of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = range(1, 40)\n",
    "SSE = []\n",
    "for k in numClusters:\n",
    "    k_means = cluster.KMeans(n_clusters=k, n_init=10)\n",
    "    k_means.fit(db2_preprocessed)\n",
    "    SSE.append(k_means.inertia_)\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.plot(numClusters, SSE, marker='o', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing n_clusters=9 looks like a good fit on the elbow line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Cluster\n",
    "Split data and keep a small portion (10%) for analyzing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ind = int(len(db2_preprocessed) * 0.9)\n",
    "data_train = db2_preprocessed[:split_ind]\n",
    "data_test = db2_preprocessed[split_ind:]\n",
    "print(f\"Train samples: {len(data_train)}, Analysis samples: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the k-means clustering model, we fit the training data above in order to find unique clusters from the dataset. <br />Then, we can cluster each movie title to the closest cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = cluster.KMeans(n_clusters=n_clusters, max_iter=100, n_init=10, random_state=1)\n",
    "k_means.fit(data_train) \n",
    "labels = k_means.labels_\n",
    "print(\"Unique cluster ids: \", np.unique(labels))\n",
    "clusters_train_df = pd.DataFrame(labels, index=db2.title[:split_ind], columns=['Cluster ID'])\n",
    "clusters_train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append 'genre' column to analyze our clusters on input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append 'genre' column to analyze our clusters\n",
    "clusters_train_df['genre'] = db2.genre[:split_ind].values\n",
    "clusters_train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze clusters for genre composition.\n",
    "Ideally clusters should show grouping of similar genres. </br>\n",
    "Our clusters have good genre composition as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genre composition for cluster 1\")\n",
    "print(clusters_train_df.groupby(['Cluster ID', 'genre']).size()[1])\n",
    "\n",
    "print(\"Genre composition for cluster 2\")\n",
    "print(clusters_train_df.groupby(['Cluster ID', 'genre']).size()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to visualize our clusters in 2 dimensions\n",
    "We project the training data to 2D with PCA and then color each sample (movie) with the cluster id color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
    "\n",
    "data_train_2D = pd.DataFrame(KernelPCA(n_components=2, kernel='linear').fit_transform(data_train), columns=['PC1', 'PC2'])\n",
    "data_train_2D.plot.scatter(x='PC1', y='PC2', c=clusters_train_df['Cluster ID'], colormap='tab20c', ax = axes[0], subplots=True)\n",
    "\n",
    "color_labels = clusters_train_df['genre'].unique()\n",
    "rgb_values = sns.color_palette(\"Set2\", 11)\n",
    "color_map = dict(zip(color_labels, rgb_values))\n",
    "data_train_2D.plot.scatter(x='PC1', y='PC2', c=clusters_train_df['genre'].map(color_map), \\\n",
    "                           title='PCA projection of training data colored by cluster ID (left) and by genre (right)', \\\n",
    "                           ax = axes[1], subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "centroids = k_means.cluster_centers_\n",
    "centroids_df = pd.DataFrame(centroids,columns=data_train.columns)\n",
    "pd.DataFrame(KernelPCA(n_components=2, kernel='rbf').fit_transform(centroids_df), columns=['PC1', 'PC2']) \\\n",
    "    .plot.scatter(x='PC1', y='PC2', title=\"2D Visualization (PCA projection) of KMeans centroids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply trained KMeans algorithm to the held out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster labels for unseen movies using trained KMeans\n",
    "labels = k_means.predict(data_test)\n",
    "labels = labels.reshape(-1,1)\n",
    "# Print SSE on test\n",
    "print(\"Model inertia: \", k_means.inertia_)\n",
    "\n",
    "# Create a dataframe that has new movies and their cluster assignment\n",
    "newmovies = db[split_ind:].copy()\n",
    "newmovies['Cluster ID'] = labels\n",
    "print(\"Cluster allocation for new, unused in training movies\")\n",
    "newmovies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Analysis on the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply encoding to the dataset in order to change the text string values into a integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_text_index(imdb_dataset, 'title_type')\n",
    "encode_text_index(imdb_dataset, 'mpaa_rating')\n",
    "encode_text_index(imdb_dataset, 'critics_rating')\n",
    "encode_text_index(imdb_dataset, 'audience_rating')\n",
    "encode_text_index(imdb_dataset, 'best_pic_nom')\n",
    "encode_text_index(imdb_dataset, 'best_pic_win')\n",
    "encode_text_index(imdb_dataset, 'best_actor_win')\n",
    "encode_text_index(imdb_dataset, 'best_actress_win')\n",
    "encode_text_index(imdb_dataset, 'best_dir_win')\n",
    "encode_text_index(imdb_dataset, 'top200_box')\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the dataset so clustering plot is more readable & displaying the dendrogram for single link hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "Y = imdb_dataset['genre']\n",
    "X = imdb_dataset.drop(['id', 'title','genre', 'runtime', 'studio', 'thtr_rel_year', 'thtr_rel_month', 'thtr_rel_day', 'dvd_rel_year', 'dvd_rel_month', 'dvd_rel_day',\n",
    "               'director', 'actor1', 'actor2', 'actor3', 'actor4', 'actor5', 'imdb_url', 'rt_url'],axis=1)\n",
    "\n",
    "# Minimizing the rows by choosing 40 random movies\n",
    "names = imdb_dataset['title'].sample(n=40, random_state=0)\n",
    "X = X.sample(n=40, random_state=0)\n",
    "\n",
    "Z = hierarchy.linkage(X.values, 'single')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we do hierarchical clustering with complete linkage which calculates gets the max distance between clusters then displaying the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X.values, 'complete')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we do the group average method of hierarchical clustering getting average distance instead between cluster points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X.values, 'average')\n",
    "dn = hierarchy.dendrogram(Z,labels=names.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for text mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the assignment 4 directions\n",
    "text_dataset = [ 'Now for manners use has company believe parlors.',\n",
    "'Least nor party who wrote while did. Excuse formed as is agreed admire so on result parish.',\n",
    "'Put use set uncommonly announcing and travelling. Allowance sweetness direction to as necessary.',\n",
    "'Principle oh explained excellent do my suspected conveying in.',\n",
    "'Excellent you did therefore perfectly supposing described. ',\n",
    "'Its had resolving otherwise she contented therefore.',\n",
    "'Afford relied warmth out sir hearts sister use garden.',\n",
    "'Men day warmth formed admire former simple.',\n",
    "'Humanity declared vicinity continue supplied no an. He hastened am no property exercise of. ' ,\n",
    "'Dissimilar comparison no terminated devonshire no literature on. Say most yet head room such just easy. ']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vector Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer picks out unique words and places their count in a vector. We then take vectorizer and format it to a matrix using a transform function. When we print out the matrix, it will display each unique word in a column and how many times each document (row) has used them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n",
    "#min_df is set to 2 to keep the matrix from being too cluttered.\n",
    "vectorizer = sk_text.CountVectorizer(min_df=2)\n",
    "#vectorizer = sk_text.CountVectorizer(stop_words = 'english')\n",
    "\n",
    "#min_df: ignore terms that have a document frequency < min_df.\n",
    "\n",
    "#format the vectorizer into a readable matrix.\n",
    "matrix = vectorizer.fit_transform(text_dataset)\n",
    "\n",
    "print(type(matrix))          # Compressed Sparse Row matrix\n",
    "print(matrix.toarray())        #  convert it to numpy array\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vector Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF calculates how relevant a word is to a text. Vectorizer takes the unique words and evaluates them based on the number of times a word appears compared to the frequency in the dataset. We then format the vector into a matrix and print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sk_text.TfidfVectorizer(\n",
    "                             #stop_words='english',\n",
    "                             #max_features = 1000,\n",
    "                             min_df=2)\n",
    "#min_df is set to 2 to prevent the matrix from being too cluttered.\n",
    "\n",
    "#max_features:  build a vocabulary that only consider the top max_features features ordered by term frequency across the corpus.\n",
    "\n",
    "matrix = vectorizer.fit_transform(text_dataset)\n",
    "\n",
    "print(type(matrix))          # Compressed Sparse Row matrix\n",
    "print(matrix.toarray())        #  convert it to numpy array\n",
    "np.set_printoptions(precision=4)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4) Tfidf (term frequency-inverse document frequency) is a measure of how frequent a word appears in a set of documents. It is generally used in text analysis algorithms and for document searching. For example, Google search uses Tfidf for text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Implementation\n",
    "In this section, we will be performing ANN techniques on the Admission dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def change_to_binary_values(df, col_name):\n",
    "    df[col_name] = (df[col_name] > df[col_name].median()).astype('int')\n",
    "    \n",
    "#Function to normalize columns\n",
    "def normalize_numeric_minmax(df, name):\n",
    "        df[name] = ((df[name] - df[name].min()) / (df[name].max() - df[name].min())).astype(np.float32)\n",
    "        \n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "# def encode_text_dummy(df, name):\n",
    "#     dummies = pd.get_dummies(df[name])\n",
    "#     for x in dummies.columns:\n",
    "#         dummy_name = \"{}-{}\".format(name, x)\n",
    "#         df[dummy_name] = dummies[x]\n",
    "#     df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "import collections\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.abc.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Admission dataset and displaying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "admission_dataset = pd.read_csv(\"./Admission_Predict_Ver1.1_small_data_set_for_Linear_Regression-1.csv\")\n",
    "admission_dataset = admission_dataset.drop(columns=['Serial No.'])\n",
    "admission_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing foe ANN: Normalize numerical predictors and binarize the targets for classification.\n",
    "For a numerical variable X that takes values in the range [a, b] where a < b, </br>we normalize the measurements by subtracting a and dividing by b − a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_numeric_minmax(admission_dataset, 'GRE Score')\n",
    "normalize_numeric_minmax(admission_dataset, 'TOEFL Score')\n",
    "normalize_numeric_minmax(admission_dataset, 'University Rating')\n",
    "normalize_numeric_minmax(admission_dataset, 'SOP')\n",
    "normalize_numeric_minmax(admission_dataset, 'LOR ')\n",
    "normalize_numeric_minmax(admission_dataset, 'CGPA')\n",
    "change_to_binary_values(admission_dataset, 'Chance of Admit ')\n",
    "admission_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all input features should be in [0, 1] range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down below, we will be splitting up the admission dataset into training and testing that will be used to calculate our Mean Sum of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = admission_dataset.drop('Chance of Admit ', axis=1)\n",
    "y = admission_dataset['Chance of Admit ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our testing size is sitting at 20% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with sklearn MLPClassifier with 2 hidden layers\n",
    "Once we have stadardized our training and test dataset, we then apply MLP (Multi-layer perceptron) Classification which comes from the neural network sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_score = mlp.predict(X_test_scaled)\n",
    "y_pred = y_pred_score > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on test data is %.2f' % (accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Rejected', 'Admitted'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the prediction that uses the MLP Classifier, we then find the Mean sum squarred error. The mean sum squared error shows to us that the error for this dataset when comparing both the tested variable and the predicted variable has a low error value when predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Keras ANN Prediction and Classification\n",
    "\n",
    "Down below, we will be importing keras for using ANN methods. Make sure that keras and tensorflow is installed in order for the code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "admission_dataset = pd.read_csv(\"./Admission_Predict_Ver1.1_small_data_set_for_Linear_Regression-1.csv\")\n",
    "admission_dataset = admission_dataset.drop(columns=\"Serial No.\")\n",
    "admission_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Chance of Admit to become binary values. This is due to ANN using [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_to_binary_values(admission_dataset, 'Chance of Admit ')\n",
    "admission_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the Chance of Admit column into 'yes' and 'no' values and storing it into a variable called classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_dataset['Chance of Admit '].replace((1, 0), ('yes', 'no'), inplace=False)\n",
    "classes = encode_text_index(admission_dataset, 'Chance of Admit ')\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize numerical columns and separate features and targets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_numeric_minmax(admission_dataset, 'GRE Score')\n",
    "normalize_numeric_minmax(admission_dataset, 'TOEFL Score')\n",
    "normalize_numeric_minmax(admission_dataset, 'University Rating')\n",
    "normalize_numeric_minmax(admission_dataset, 'SOP')\n",
    "normalize_numeric_minmax(admission_dataset, 'LOR ')\n",
    "normalize_numeric_minmax(admission_dataset, 'CGPA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test data set that will take 40 random rows from the admission set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a random sample of 40 rows for our testing\n",
    "split_index = 460\n",
    "test_data = admission_dataset[split_index:]\n",
    "train_data = admission_dataset[:split_index]\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = to_xy(train_data, 'Chance of Admit ')\n",
    "testX, testY = to_xy(test_data, 'Chance of Admit ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Neural network with 2 hidden Dense layers with ReLU activations and a final Softmax layer to predict one hot encoded targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, input_dim = X.shape[1], activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss and optimizer and fit model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model.fit(X, y, verbose=0, epochs=1000, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model for generating our predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testX)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred, axis=1)\n",
    "true = np.argmax(testY, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputting our class (which is the Chance of Admit column) and observing our predicted set from our actual set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted classes: \", classes[pred])\n",
    "print(\"True classes: \", classes[true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the accuracy from our ANN technique as well as the classification report. As the value is somewhat above average, this tells us that our predicted values are in line with our actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on test data is %.2f' % (accuracy_score(true, pred)))\n",
    "print(classification_report(true,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
