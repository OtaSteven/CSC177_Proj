{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports and encode values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(data, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data[name] = le.fit_transform(data[name])\n",
    "    return le.classes_\n",
    "\n",
    "def classification_report_and_cm(y_test, y_pred, display_labels):\n",
    "  print('Accuracy on test data is %.2f' % (accuracy_score(y_test, y_pred)))\n",
    "  print('F1 score on test data is %.2f' % (f1_score(y_test, y_pred)))\n",
    "  print('Precision Score on test data is %.2f' % (precision_score(y_test, y_pred)))\n",
    "  print('Recall score on test data is %.2f' % (recall_score(y_test, y_pred)))\n",
    "  print(classification_report(y_test,y_pred))\n",
    "\n",
    "  cm = confusion_matrix(y_test, y_pred)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "  disp.plot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Classification Models on Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data: drop columns not used in classification, fill N/A values with mean and encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = pd.read_csv(\"./titanic_train.csv\")\n",
    "# Drop passenger name, ticket number, cabin, embarked.\n",
    "titanic_train.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], inplace=True)\n",
    "# Replace NaNs in Age with mean\n",
    "titanic_train['Age'].fillna(titanic_train['Age'].mean(), inplace=True)\n",
    "# Encode 'Sex' column\n",
    "titanic_train['Sex'] = pd.get_dummies(titanic_train['Sex'], drop_first=True)\n",
    "titanic_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation Matrix for Titanic Data attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = titanic_train.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at 'Survived' column which is our target for prediction, we can see that the cabin class ('Pclass' column), the gender('Sex' column) and the ticket price ('Fare' column) are the most important features. The gender and ticket class are anticorrelated with survival since gender is 0-female, 1-male and for class, increasing class number decreased chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features pairplot with Survived hue to get insights on the pairwise relationships within the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Paired plot using seaborn\n",
    "sns.pairplot(titanic_train[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']],\n",
    "             hue='Survived', diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot we can see that when Sex=1(Male) only Age < 10 or so had good chances of survival.\n",
    "Also when Sex=0(Female) and Fare > 25 passengers had good chances of survival.\n",
    "Passengers with Pclass=1 had good chances of survival, much better than Pclass=2 or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training features and labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_titanic = titanic_train[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']].values\n",
    "y_titanic = titanic_train['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, test_size=0.3, random_state=0)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply decision tree similar to Tutorial_6_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "titanic_clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "titanic_clf = titanic_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "titanic_predictions = titanic_clf.predict(X_test)\n",
    "titanic_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix for titanic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_and_cm(y_test, titanic_predictions, [\"Not survived\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree performance on Titanic data with varying tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "treeDepth = [3, 4, 5]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "fig, ax_roc = plt.subplots(1, 1, figsize=(11, 5))\n",
    "ax_roc.grid(linestyle=\"--\")\n",
    "\n",
    "for k in treeDepth:\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=\"Decision Tree Classifier depth=%d\" % k)\n",
    "\n",
    "ax_roc.set_title(\"Receiver Operating Characteristic (ROC) curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that depth = 3, blue line with AUC=0.87 seems to be the best choice between 3, 4, and 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier on Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numNeighbors = [1, 5, 10, 15, 20, 25, 30, 40]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "for k in numNeighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    knn_pred = clf.predict(X_test)\n",
    "    knn_pred_train = clf.predict(X_train)\n",
    "    # print(knn_pred)\n",
    "    testAcc.append(accuracy_score(y_test, knn_pred))\n",
    "    trainAcc.append(accuracy_score(y_train,knn_pred_train))\n",
    "\n",
    "plt.plot(numNeighbors, testAcc,'bv--',numNeighbors, trainAcc, 'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuacy'])\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "print('Best accuracy score: %.3f' % max(testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of neighbors for the KNN classifer on Titanic Data is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion on Classification experiments on Titanic dataset\n",
    "Best DecisionTreeClassifier with (criterion='entropy', max_depth=3) obtains an Accuracy score of <b>0.82</b> on test dataset.\n",
    "Best KNeighborsClassifier with (n_neighbors=20, metric='minkowski', p=2) obtains an Accuracy score of 0.746 on test dataset.\n",
    "\n",
    "In our experiments on Titanic dataset DecisionTreeClassifier performed better than KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Classification using the Churn Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data\n",
    "Here, we are taking the churn dataset and applying preprocessing and converting rows with String data type into integer types. Aterwards, we separate the dataset into train and testing and measure the accuracy of the trained models. Using a classification decision tree, we will be able to determine what features are uncessary and what are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "# Open .csv\n",
    "filename_read = os.path.join(path,\"Churn_Modelling.csv\")\n",
    "churn_Data = pd.read_csv(filename_read, na_values=['NA','?'])\n",
    "churn_Data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for duplicates and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = churn_Data.duplicated()\n",
    "print('Number of duplicate rows = %d' % (dups.sum()))\n",
    "print('Number of empty values in a row = %d' % (churn_Data.isnull().any(axis = 1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Here, we are able to identify that the classification label for this dataset uses 'Exited' columns, signifying if the user has left or not. <br />\n",
    "We then take all the features from the row, dropping the unnecessary columns for our classfication (which are the RowNumber, CustomerId, and Surname) as they provide no signal as to if the user has exited or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary features from the dataset.\n",
    "churn_Data = churn_Data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "encode_text_index(churn_Data, 'Geography')\n",
    "encode_text_index(churn_Data, 'Gender')\n",
    "churn_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_Data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation Matrix for Churn Data attributes\n",
    "We can observe that 'Age' is the most correlated with 'Exited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = churn_Data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot of some features to see relationships between features\n",
    "We can see that for example ('NumOfProducts', 'CreditScore') pair plot shows a strong separation of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(churn_Data[['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'IsActiveMember', 'Exited']], \n",
    "            hue='Exited', diag_kind=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using all the features given to us to create our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "feature_cols = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "x = churn_Data[feature_cols]\n",
    "y = churn_Data.Exited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training one Decision Tree using all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building decision trees involves making a predictive model to recursively divide feature spaces into smaller and smaller sections. \n",
    "Decision trees improves our ability to understand how the model arrived at specific classifications by mimicking the human decision-making patterns. Since they can work on both numerical and categorical features without needing much data preprocessing they are extremely convenient to use. Decision trees also can show a measure of feature importance, which helps us identify which features are more influential in making decisions, allowing us to tweak weights and aides in feature selection. Furthermore, they can identify non-linear relationships in the data, since they do not make any assumptions about the linearity of the data. This makes them useful for datasets with intricate relationships between features and the target variable.\n",
    "\n",
    "Decision trees have a tendency to overfit, especially when they \"grow too deep\" or if the dataset is noisy. They are also unstable, with massive changes in the tree if the dataset changes. They also have tendency to favor features with a lot of unique values because they can create more branches which may result in more specific rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model accuracy\n",
    "As we can see here, using all of the features resulted in us have a relatively high accuracy which make sense as we are using all of the provided features to identify between Exited (1) or Not (0) users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_and_cm(y_test, y_pred, ['Not Exited', 'Exited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using graphviz, we are able to create the decision tree image of all the features from the churn dataset. As seen below, the tree has an imbalance and is having trouble classifying which account has exited or not (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(clf, feature_names=x.columns, class_names=['0','1'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (continued): Training one Decision Tree with less features, selected based on previous insights\n",
    "The code below sets up the decision tree classifier, trains it using feature columns: 'CreditScore', 'Age', 'Balance', 'NumOfProducts', and uses 'Exited' as the target variable.<br />\n",
    "It is shown below that they can be used to train a classifier that achieves 0.83 accuracy when predicting if an account has Exited or Not (1 or 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['CreditScore', 'Age', 'Balance', 'NumOfProducts']\n",
    "\n",
    "x = churn_Data[feature_cols]\n",
    "y = churn_Data.Exited\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "x_train_sel, x_test_sel, y_train_sel, y_test_sel = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DecisionTreeClassifier and display the accuracy achieved by the decision tree model in predicting the 'Exited' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train_sel, y_train_sel)\n",
    "#Predict the response for test dataset\n",
    "y_pred_sel = clf.predict(x_test_sel)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "classification_report_and_cm(y_test_sel, y_pred_sel, ['Not Exited', 'Exited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Decision tree classifier retained most of the Accuracy (0.83 vs 0.86) by using a smaller number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(clf, feature_names=x.columns, class_names=['0','1'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree performance when varying tree depth\n",
    "\n",
    "By looking at the Accuracy vs max_depth plot below we observe that best depth is around [4, 5, 6].<br />\n",
    "With larger max_depth the DecisionTreeClassifier starts to have lower accuracy on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeDepth = [3, 4, 5, 6, 7, 8, 9, 10, 11, 15]\n",
    "treeTestAcc = []\n",
    "treeTrainAcc = []\n",
    "\n",
    "for k in treeDepth:\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=k)\n",
    "    clf.fit(x_train, y_train)\n",
    "    tree_pred = clf.predict(x_test)\n",
    "    tree_pred_train = clf.predict(x_train)\n",
    "    treeTestAcc.append(accuracy_score(y_test, tree_pred))\n",
    "    treeTrainAcc.append(accuracy_score(y_train,tree_pred_train))\n",
    "     #display accuracy of test \n",
    "    print (\"Accuracy for k=%d: \"%k,  accuracy_score(y_test, tree_pred))  \n",
    "\n",
    "#display a plot\n",
    "plt.plot(treeDepth, treeTestAcc,'bv--',treeDepth,treeTrainAcc,'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply logistic regression of varying strength to the train and test. We use a range of numbers for C, starting off from a smaller value for strong regularization to greater values. This is to ensure that we are able to obtain the accuracy of our train and test base off the varying regularization strengths that the test and train goes through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "#C = Inverse of regularization strength;  smaller values specify stronger regularization.\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "\n",
    "#finding test accuracy and train accuracy\n",
    "LRtestAcc = []\n",
    "LRtrainAcc = []\n",
    "\n",
    "#for loop that applies logistic regression at different C levels and stores values to lists\n",
    "for param in C:\n",
    "    clf = LogisticRegression(C=param)\n",
    "    clf.fit(x_train,y_train)\n",
    "    log_reg_pred = clf.predict(x_test)\n",
    "    log_reg_pred_train = clf.predict(x_train)\n",
    "    # print(log_reg_pred)\n",
    "    LRtestAcc.append(accuracy_score(y_test, log_reg_pred))\n",
    "    LRtrainAcc.append(accuracy_score(y_train,log_reg_pred_train))\n",
    "     #display accuracy of test \n",
    "    print (\"Accuracy for C=%.2f: \"%param,  accuracy_score(y_test, log_reg_pred))  \n",
    "\n",
    "#display a plot\n",
    "plt.plot(C, LRtestAcc,'bv--',C,LRtrainAcc,'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.ylim(0.75, 0.85)\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "clf_NB.fit(x_train, y_train)\n",
    "NB_pred = clf_NB.predict(x_test)\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(y_test, NB_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Classifer with Linear kernel\n",
    "C = determines the strength of regularization (regularization is inversely proportional to C)<br />\n",
    "We apply normalization for SVM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "svmClassifier = make_pipeline(StandardScaler(), svm.SVC(kernel='linear', C=1))\n",
    "svmClassifier.fit(x_train, y_train)\n",
    "y_svmPred = svmClassifier.predict(x_test)\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(y_test, y_svmPred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Classifer with Non-linear (RBF) kernel\n",
    "This code block trains the non-linear kernel SVM classifier with different regularization strengths C<br />\n",
    "After running, a graph is generated for the test and train accuracy depending on the C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "\n",
    "C = [0.01, 0.1, 0.5, 0.8, 1, 5, 10, 20]\n",
    "\n",
    "\n",
    "SVMLtestAcc = []\n",
    "SVMLtrainAcc = []\n",
    "\n",
    "for param in C:\n",
    "    # clf = SVC(C=param,kernel='rbf', gamma='auto')\n",
    "    clf = make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma='auto', C=param))\n",
    "    clf.fit(x_train,y_train)\n",
    "    svml_pred = clf.predict(x_test)\n",
    "    svml_pred_train = clf.predict(x_train)\n",
    "    # print(svml_pred)\n",
    "    print (\"Accuracy on test dataset for C=%.2f: \"%param,  accuracy_score(y_test, svml_pred))  \n",
    "    SVMLtestAcc.append(accuracy_score(y_test, svml_pred))\n",
    "    SVMLtrainAcc.append(accuracy_score(y_train,svml_pred_train))\n",
    "\n",
    "plt.plot(C, SVMLtestAcc,'ro--', C,SVMLtrainAcc,'bv--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 1)\n",
    "plt.ylabel('Accuracy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that SVM with radial basis kernel overfits on the training set when regularization is not strong enough.\n",
    "For sklearn.svm.SVC the strength of the regularization is inversely proportional to C.\n",
    "In our tests the best value for C seems to be between 0.5 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN) Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numNeighbors = [5, 10, 20, 30, 40]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "for k in numNeighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "    clf.fit(x_train, y_train)\n",
    "    knn_pred = clf.predict(x_test)\n",
    "    knn_pred_train = clf.predict(x_train)\n",
    "    # print(knn_pred)\n",
    "    testAcc.append(accuracy_score(y_test, knn_pred))\n",
    "    trainAcc.append(accuracy_score(y_train,knn_pred_train))\n",
    "    print('Accuracy on test data using k=%i is %.2f' % (k, accuracy_score(y_test, knn_pred)))\n",
    "    # print('Accuracy on train data using k=%i is %.2f' % (k, accuracy_score(y_train, knn_pred_train)))\n",
    "    \n",
    "plt.plot(numNeighbors, testAcc,'bv--',numNeighbors, trainAcc, 'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuacy'])\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-value that seems to be an accurate representation of the model is when k >= 30 as we can see on the graph. When comparing the two lines, training and testing, we see that the model stops and stay on a constant value on the graph 0.79 and 0.8. Upon analyizing this, we can tell that the accuracy of the model using the KNN technique is around 0.79 ~ 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion on Classification using the Churn Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with DecisionTreeClassifier, LogisticRegression, NaiveBayes, SVM with both linear and non-linear kernels and lastly with KNNClassifier.<br />\n",
    "The best classifiers in our experiments on Churn Data set are:<br />\n",
    "SVM with 'rbf' kernel (C=5)<br />\n",
    "DecisionTreeClassifier with max_depth=5\n",
    "\n",
    "Both obtain 0.86 accuracy on the split test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmClassifierRbf = make_pipeline(StandardScaler(), svm.SVC(kernel='rbf', gamma='auto', C=1))\n",
    "svmClassifierRbf.fit(x_train, y_train)\n",
    "svmRbfPred = svmClassifierRbf.predict(x_test)\n",
    "classification_report_and_cm(y_test, svmRbfPred, ['Not Exited', 'Exited'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
