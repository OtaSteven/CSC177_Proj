{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: Classification\n",
    "\n",
    "The following tutorial contains Python examples for solving classification problems. You should refer to the Chapters 3 and 4 of the \"Introduction to Data Mining\" book to understand some of the concepts introduced in this tutorial.\n",
    "\n",
    "Classification is the task of predicting a nominal-valued attribute (known as class label) based on the values of other attributes (known as predictor variables). The goals for this tutorial are as follows:\n",
    "1. To provide examples of using different classification techniques from the scikit-learn library package.\n",
    "2. To demonstrate the problem of model overfitting.\n",
    "\n",
    "Read the step-by-step instructions below carefully. To execute the code, click on the corresponding cell and press the SHIFT-ENTER keys simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if the data is not categorical, i.e. if the data is continous numerical value? Can we use tree for the same? \n",
    "\n",
    "Answer is Yes.\n",
    "\n",
    "We can use the Decision Tree Regressor Model to work on the continous numerical value.\n",
    "\n",
    "Here is the link for the DecisionTreeRegressor Model.\n",
    "\n",
    "DecisionTreeRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(\n",
    "[['Asset Flip', 100, 1000],\n",
    "['Text Based', 500, 3000],\n",
    "['Visual Novel', 1500, 5000],\n",
    "['2D Pixel Art', 3500, 8000],\n",
    "['2D Vector Art', 5000, 6500],\n",
    "['Strategy', 6000, 7000],\n",
    "['First Person Shooter', 8000, 15000],\n",
    "['Simulator', 9500, 20000],\n",
    "['Racing', 12000, 21000],\n",
    "['RPG', 14000, 25000],\n",
    "['Sandbox', 15500, 27000],\n",
    "['Open-World', 16500, 30000],\n",
    "['MMOFPS', 25000, 52000],\n",
    "['MMORPG', 30000, 80000]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows by : and column 1\n",
    "# by 1:2 representing features\n",
    "X = dataset[:, 1:2].astype(int)\n",
    "y = dataset[:, 2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict([[3750]])\n",
    "print(\"Predicted price: % d\\n\"% y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arange for creating a range of values\n",
    "# from min value of X to max value of X\n",
    "# with a difference of 0.01 between two\n",
    "# consecutive values\n",
    "X_grid = np.arange(min(X), max(X), 0.01)\n",
    "\n",
    "# reshape for reshaping the data into\n",
    "# a len(X_grid)*1 array, i.e. to make\n",
    "# a column out of the X_grid values\n",
    "\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Profit to Production Cost (Decision Tree Regressor)')\n",
    "plt.xlabel('Production Cost')\n",
    "plt.ylabel('Profit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz  \n",
    "\n",
    "dot_data = tree.export_graphviz(regressor, out_file =None,feature_names =['Production Cost'])  \n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Decsision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertebrate Dataset\n",
    "\n",
    "We use a variation of the vertebrate data described in Example 3.1 of Chapter 3. Each vertebrate is classified into one of 5 categories: mammals, reptiles, birds, fishes, and amphibians, based on a set of explanatory attributes (predictor variables). Except for \"name\", the rest of the attributes have been converted into a *one hot encoding* binary representation. To illustrate this, we will first load the data into a Pandas DataFrame object and display its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('data/vertebrate.csv',header='infer')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limited number of training examples, suppose we convert the problem into a binary classification task (mammals versus non-mammals). We can do so by replacing the class labels of the instances to *non-mammals* except for those that belong to the *mammals* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Class'] = data['Class'].replace(['fishes','birds','amphibians','reptiles'],'non-mammals')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paired plot using seaborn\n",
    "sns.set()\n",
    "sns.pairplot(data[['Warm-blooded', 'Gives Birth', 'Aquatic Creature', 'Aerial Creature', 'Has Legs','Hibernates','Class']],\n",
    "             hue='Class', diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply Pandas cross-tabulation to examine the relationship between the Warm-blooded and Gives Birth attributes with respect to the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([data['Warm-blooded'],data['Gives Birth']],data['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that it is possible to distinguish mammals from non-mammals using these two attributes alone since each combination of their attribute values would yield only instances that belong to the same class. For example, mammals can be identified as warm-blooded vertebrates that give birth to their young. Such a relationship can also be derived using a decision tree classifier, as shown by the example given in the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "In this section, we apply a decision tree classifier to the vertebrate dataset described in the previous subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "Y = pd.DataFrame(data, columns=['Class'])\n",
    "X = data.drop(['Name','Class'],axis=1)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth=3)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding commands will extract the predictor (X) and target class (Y) attributes from the vertebrate dataset and create a decision tree classifier object using entropy as its impurity measure for splitting criterion. The decision tree class in Python sklearn library also supports using 'gini' as impurity measure. The classifier above is also constrained to generate trees with a maximum depth equals to 3. Next, the classifier is trained on the labeled data using the fit() function. \n",
    "\n",
    "We can plot the resulting decision tree obtained after training the classifier. To do this, you must first install both graphviz (http://www.graphviz.org) and its Python interface called pydotplus (http://pydotplus.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=X.columns,  \n",
    "                      class_names=['mammals','non-mammals'],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, suppose we apply the decision tree to classify the following test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = [['gila monster',0,0,0,0,1,1,'non-mammals'],\n",
    "           ['platypus',1,0,0,0,1,1,'mammals'],\n",
    "           ['owl',1,0,0,1,1,0,'non-mammals'],\n",
    "           ['dolphin',1,1,1,0,0,0,'mammals']]\n",
    "testData = pd.DataFrame(testData, columns=data.columns)\n",
    "testData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the predictor and target class attributes from the test data and then apply the decision tree classifier to predict their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = pd.DataFrame(testData, columns=['Class'])\n",
    "testX = testData.drop(['Name','Class'],axis=1)\n",
    "\n",
    "predY = clf.predict(testX)\n",
    "predictions = pd.concat([testData['Name'],pd.Series(predY,name='Predicted Class')], axis=1)\n",
    "predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for platypus, which is an egg-laying mammal, the classifier correctly predicts the class label of the test examples. We can calculate the accuracy of the classifier on the test data as shown by the example given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score,recall_score,classification_report\n",
    "\n",
    "confusion = confusion_matrix(testY, predY)\n",
    "print(confusion)\n",
    "\n",
    "plot_confusion_matrix(confusion, data.Class.unique(), title='Confusion matrix', cmap=plt.cm.Blues)\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(testY, predY)))\n",
    "print('F1 score on test data is %.2f' % (f1_score(testY, predY,pos_label='mammals')))\n",
    "print('Precision Score on test data is %.2f' % (precision_score(testY, predY,pos_label='mammals')))\n",
    "print('Recall score on test data is %.2f' % (recall_score(testY, predY,pos_label='mammals')))\n",
    "print( classification_report(testY,predY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(data.Class.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section, we apply a Logistic Regression to the vertebrate dataset described in the previous subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "\n",
    "LRtestAcc = []\n",
    "LRtrainAcc = []\n",
    "\n",
    "for param in C:\n",
    "    clf = LogisticRegression(C=param)\n",
    "    clf.fit(X,Y)\n",
    "    log_reg_pred = clf.predict(testX)\n",
    "    log_reg_pred_train = clf.predict(X)\n",
    "    print(log_reg_pred)\n",
    "    LRtestAcc.append(accuracy_score(testY, log_reg_pred))\n",
    "    LRtrainAcc.append(accuracy_score(Y,log_reg_pred_train))\n",
    "    \n",
    "    \n",
    "\n",
    "plt.plot(C, LRtestAcc,'bv--',C,LRtrainAcc,'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "In this section, we apply a Naise Bayes classifier to the vertebrate dataset described in the previous subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "clf_NB.fit(X,Y)\n",
    "NB_pred = clf_NB.predict(testX)\n",
    "print(NB_pred)\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(testY, NB_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Classifier\n",
    "\n",
    "In this section, we apply a SVM classifier to the vertebrate dataset described in the previous subsection. We will also experiment with C-parameter and different kernals and see how it effects the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Linear Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "\n",
    "SVMLtestAcc = []\n",
    "SVMLtrainAcc = []\n",
    "\n",
    "\n",
    "\n",
    "for param in C:\n",
    "    clf = SVC(C=param,kernel='linear')\n",
    "    clf.fit(X,Y)\n",
    "    svml_pred = clf.predict(testX)\n",
    "    svml_pred_train = clf.predict(X)\n",
    "    print(svml_pred)\n",
    "    SVMLtestAcc.append(accuracy_score(testY, svml_pred))\n",
    "    SVMLtrainAcc.append(accuracy_score(Y,svml_pred_train))\n",
    "\n",
    "plt.plot(C, SVMLtestAcc,'ro--', C,SVMLtrainAcc,'bv--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Non Linear Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    " \n",
    "\n",
    "SVMLtestAcc = []\n",
    "SVMLtrainAcc = []\n",
    "\n",
    "\n",
    "\n",
    "for param in C:\n",
    "    clf = SVC(C=param,kernel='rbf',gamma='auto')\n",
    "    clf.fit(X,Y)\n",
    "    svml_pred = clf.predict(testX)\n",
    "    svml_pred_train = clf.predict(X)\n",
    "    print(svml_pred)\n",
    "    SVMLtestAcc.append(accuracy_score(testY, svml_pred))\n",
    "    SVMLtrainAcc.append(accuracy_score(Y,svml_pred_train))\n",
    "\n",
    "plt.plot(C, SVMLtestAcc,'ro--', C,SVMLtrainAcc,'bv--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## K Nearest Neighbor (KNN) Classifier\n",
    "\n",
    "In this section, we apply a K - Nearest Neighbor classifier to the vertebrate dataset described in the previous subsection. We will also look at how the K value effect the performance of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numNeighbors = [1, 5, 10]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "for k in numNeighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "    clf.fit(X, Y)\n",
    "    knn_pred = clf.predict(testX)\n",
    "    knn_pred_train = clf.predict(X)\n",
    "    print(knn_pred)\n",
    "    testAcc.append(accuracy_score(testY, knn_pred))\n",
    "    trainAcc.append(accuracy_score(Y,knn_pred_train))\n",
    "\n",
    "plt.plot(numNeighbors, testAcc,'bv--',numNeighbors, trainAcc, 'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuacy'])\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "import collections\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/vertebrate.csv',header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop('Name', axis=1)\n",
    "data_df['Class'] = data_df['Class'].replace(['fishes','birds','amphibians','reptiles'],'non-mammals')\n",
    "Classes = encode_text_index(data_df,'Class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testData = [['gila monster',0,0,0,0,1,1,'non-mammals'],\n",
    "           ['platypus',1,0,0,0,1,1,'mammals'],\n",
    "           ['owl',1,0,0,1,1,0,'non-mammals'],\n",
    "           ['dolphin',1,1,1,0,0,0,'mammals']]\n",
    "testData = pd.DataFrame(testData, columns=data.columns)\n",
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = testData.drop('Name', axis=1)\n",
    "encode_text_index(testData,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = to_xy(data_df,'Class')\n",
    "testX, testY = to_xy(testData,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = X.shape[1], activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model.fit(X,Y,verbose=2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testX)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.argmax(testY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes[true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on test data is %.2f' % (accuracy_score(true, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
