{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup imports and encode values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(data, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data[name] = le.fit_transform(data[name])\n",
    "    return le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models on Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = pd.read_csv(\"./titanic_train.csv\")\n",
    "# Drop passenger name, ticket number, cabin, embarked.\n",
    "titanic_train.drop(columns=['Name', 'Ticket', 'Cabin', 'Embarked'], inplace=True)\n",
    "# Replace NaNs in Age with mean\n",
    "titanic_train['Age'].fillna(titanic_train['Age'].mean(), inplace=True)\n",
    "# Encode 'Sex' column\n",
    "titanic_train['Sex'] = pd.get_dummies(titanic_train['Sex'], drop_first=True)\n",
    "titanic_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_titanic = titanic_train[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']].values\n",
    "y_titanic = titanic_train['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, test_size=0.3, random_state=0)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply decision tree similar to Tutorial_6_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "titanic_clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "titanic_clf = titanic_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "titanic_predictions = titanic_clf.predict(X_test)\n",
    "titanic_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrix for titanic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score,classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def classification_report_and_cm(y_test, y_pred):\n",
    "  print('Accuracy on test data is %.2f' % (accuracy_score(y_test, y_pred)))\n",
    "  print('F1 score on test data is %.2f' % (f1_score(y_test, y_pred)))\n",
    "  print('Precision Score on test data is %.2f' % (precision_score(y_test, y_pred)))\n",
    "  print('Recall score on test data is %.2f' % (recall_score(y_test, y_pred)))\n",
    "  print( classification_report(y_test,y_pred))\n",
    "\n",
    "  cm = confusion_matrix(y_test, y_pred)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= [\"Not survived\", \"Survived\"])\n",
    "  disp.plot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_and_cm(y_test, titanic_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Correlation Matrix for Titanic Data attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = titanic_train.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree performance on Titanic data with varying tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "treeDepth = [3, 4, 5]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "fig, ax_roc = plt.subplots(1, 1, figsize=(11, 5))\n",
    "ax_roc.grid(linestyle=\"--\")\n",
    "\n",
    "for k in treeDepth:\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=\"Decision Tree Classifier depth=%d\" % k)\n",
    "\n",
    "ax_roc.set_title(\"Receiver Operating Characteristic (ROC) curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that depth = 3 is the best choice between 3, 4, and 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier on Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numNeighbors = [1, 5, 10, 15, 20, 25, 30, 40]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "for k in numNeighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    knn_pred = clf.predict(X_test)\n",
    "    knn_pred_train = clf.predict(X_train)\n",
    "    # print(knn_pred)\n",
    "    testAcc.append(accuracy_score(y_test, knn_pred))\n",
    "    trainAcc.append(accuracy_score(y_train,knn_pred_train))\n",
    "\n",
    "plt.plot(numNeighbors, testAcc,'bv--',numNeighbors, trainAcc, 'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuacy'])\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of neighbors for the KNN classifer on Titanic Data is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn Plots to Illustrate Classifier Performance for Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Paired plot using seaborn.set()\n",
    "sns.pairplot(titanic_train[['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare'\n",
    "]],\n",
    "             hue='Pclass', diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Churn Data set\n",
    "Here, we are taking the churn dataset and applying preprocessing and converting rows with String data type into integer types. Aterwards, we seperate the dataset into train and testing and predict the accuracy of the model. Using a classification decision tree, we will be able to determine what features are uncessary and what are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "# Open .csv\n",
    "filename_read = os.path.join(path,\"Churn_Modelling.csv\")\n",
    "churn_Data = pd.read_csv(filename_read, na_values=['NA','?'])\n",
    "churn_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = churn_Data.duplicated()\n",
    "print('Number of duplicate rows = %d' % (dups.sum()))\n",
    "print('Number of empty values in a row = %d' % (churn_Data.isnull().any(axis = 1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are able to identify that the classification for this dataset uses Exited, signifying if the user is has left or not. We then take all the features from the row, dropping the columns uncessary to our classfication (which are the RowNumber, CustomerId, and Surname) as they provide no classification as to if the user has exited or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary features from the dataset.\n",
    "churn_Data = churn_Data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "encode_text_index(churn_Data, 'Geography')\n",
    "encode_text_index(churn_Data, 'Gender')\n",
    "churn_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_Data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using all the features given to us to create our deicsion tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "feature_cols = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "x = churn_Data[feature_cols]\n",
    "y = churn_Data.Exited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building decision trees involves making a predictive model to recursively divide feature spaces into smaller and smaller sections. \n",
    "Decision trees improves the our ability to understand how the model arrived at specific classifications by mimicking the human decision-making patterns. Since they can work on both numerical and categorical features without needing much data preprocessing they are extremely convenient to use. Decision trees also can show a measure of feature importance, which helps us identify which features are more influential in making decisions, allowing us to tweak weights and aides in feature selection. Furthermore, They can identify non-linear relationships in the data, since they do not make any assumptions about the linearity of the data. This makes them useful for datasets with intricate relationships between features and the target variable.\n",
    "\n",
    "Decision trees have a tendency to overfit, especially when they \"grow too deep\" or if the dataset is noisy. They are also unstable, with massive changes in the tree if the dataset changes. They also have tendency to favor features with a lot of unique values because they can create more branches which may result in more specific rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train,y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model accuracy\n",
    "As we can see here, using all of the features resulted in us have a high preicsion and accuracy which make sense as we are using all of the provided features to identify between Exited (1) or Not (0) users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_and_cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using graphviz, we are able to create the decision tree image of all the features from the churn dataset. As seen below, the tree has an imbalance and is having trouble classifying which account has exited or not (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(clf, feature_names=x.columns, class_names=['0','1'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up the decision tree classifier, trains it using feature columns: 'CreditScore', 'Balance', 'NumOfProducts', and the 'Exited' target variable. We use these features as it has shown that these three features provided the most accurate score we were able to find and was able to clearly classify if an account has Exited or Not (1 or 0). It then checks its performance on a test set and displays the accuracy achieved by the decision tree model in predicting the 'Exited' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['CreditScore', 'Balance', 'NumOfProducts']\n",
    "\n",
    "x = churn_Data[feature_cols]\n",
    "y = churn_Data.Exited\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train,y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(clf, feature_names=x.columns, class_names=['0','1'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply logistic regression of varying strength to the train and test. We use a range of number for C, starting off from a smaller value for strong regularization to greater values. This is to ensure that we are able to obtain the accuracy of our train and test base off the varying regularization strengths that the test and train goes through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "#C = Inverse of regularization strength;  smaller values specify stronger regularization.\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "\n",
    "#finding test accuracy and train accuracy\n",
    "LRtestAcc = []\n",
    "LRtrainAcc = []\n",
    "\n",
    "#for loop that applies logistic regression at different C levels and stores values to lists\n",
    "for param in C:\n",
    "    clf = LogisticRegression(C=param)\n",
    "    clf.fit(x_train,y_train)\n",
    "    log_reg_pred = clf.predict(x_test)\n",
    "    log_reg_pred_train = clf.predict(x_train)\n",
    "    # print(log_reg_pred)\n",
    "    LRtestAcc.append(accuracy_score(y_test, log_reg_pred))\n",
    "    LRtrainAcc.append(accuracy_score(y_train,log_reg_pred_train))\n",
    "     #display accuracy of test \n",
    "    print (\"Accuracy for C=%.2f: \"%param,  accuracy_score(y_test, log_reg_pred))  \n",
    "\n",
    "#display a plot\n",
    "plt.plot(C, LRtestAcc,'bv--',C,LRtrainAcc,'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.ylim(0.75, 0.85)\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn Plots to Illustrate Classifier Performance for Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(churn_Data[['Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']], \n",
    "            hue= 'HasCrCard', diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "In this section, we apply the classifcation method: Naive Bayes, where we take two approaches, SVC (the linear approach) and SVM (non-linear approach). <br />\n",
    "SVC = Linear Approach <br />\n",
    "SVM = Non Linear Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "clf_NB.fit(x,y)\n",
    "NB_pred = clf_NB.predict(x_test)\n",
    "print(NB_pred)\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(y_test, NB_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Classifer\n",
    "C = determines the influence of misclassification  <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmClassifier = svm.SVC(kernel='linear', gamma='auto', C=2, max_iter=1000)\n",
    "# svmClassifier = svm.LinearSVC(C=2)\n",
    "svmClassifier.fit(x_train, y_train)\n",
    "\n",
    "y_svmPred = svmClassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(y_test, y_svmPred)))\n",
    "print(classification_report(y_test, y_svmPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix for SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_svmPred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svmClassifier.classes_)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block applies the non-linear SVM classifier which uses the C parameter to affect the accuracy and predicted results. After running, a graph is generated for the test and train accuracy depending on the C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "\n",
    "\n",
    "SVMLtestAcc = []\n",
    "SVMLtrainAcc = []\n",
    "\n",
    "for param in C:\n",
    "    clf = SVC(C=param,kernel='rbf',gamma='auto')\n",
    "    clf.fit(x_train,y_train)\n",
    "    svml_pred = clf.predict(x_test)\n",
    "    svml_pred_train = clf.predict(x_train)\n",
    "    print(svml_pred)\n",
    "    SVMLtestAcc.append(accuracy_score(y_test, svml_pred))\n",
    "    SVMLtrainAcc.append(accuracy_score(y_train,svml_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C, SVMLtestAcc,'ro--', C,SVMLtrainAcc,'bv--')\n",
    "plt.legend(['Test Accuracy','Train Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "# plt.xlim(0, 1)\n",
    "plt.ylabel('Accuracy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that SVM with radial basis kernel overfits on the training set when regularization is not strong enough.\n",
    "For sklearn.svm.SVC the strength of the regularization is inversely proportional to C.\n",
    "In our tests the best value for C seems to be between 0.01 and 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Correlation Matrix for Churn Data attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = churn_Data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree performance on Churn data with varying tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "treeDepth = [3, 7, 11]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "fig, ax_roc = plt.subplots(1, 1, figsize=(11, 5))\n",
    "ax_roc.grid(linestyle=\"--\")\n",
    "\n",
    "for k in treeDepth:\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=k)\n",
    "    clf.fit(x_train, y_train)\n",
    "    RocCurveDisplay.from_estimator(clf, x_test, y_test, ax=ax_roc, name=\"Decision Tree Classifier depth=%d\" % k)\n",
    "\n",
    "ax_roc.set_title(\"Receiver Operating Characteristic (ROC) curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN) Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numNeighbors = [5, 10, 20, 30, 40]\n",
    "testAcc = []\n",
    "trainAcc = []\n",
    "\n",
    "for k in numNeighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "    clf.fit(x_train, y_train)\n",
    "    knn_pred = clf.predict(x_test)\n",
    "    knn_pred_train = clf.predict(x_train)\n",
    "    # print(knn_pred)\n",
    "    testAcc.append(accuracy_score(y_test, knn_pred))\n",
    "    trainAcc.append(accuracy_score(y_train,knn_pred_train))\n",
    "    print('Accuracy on test data using k=%i is %.2f' % (k, accuracy_score(y_test, knn_pred)))\n",
    "    print('Accuracy on train data using k=%i is %.2f' % (k, accuracy_score(y_train, knn_pred_train)))\n",
    "    \n",
    "plt.plot(numNeighbors, testAcc,'bv--',numNeighbors, trainAcc, 'ro--')\n",
    "plt.legend(['Test Accuracy','Train Accuacy'])\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-value that seems to be an accurate representation of the model is when k >= 30 as we can see on the graph. When comparing the two lines, training and testing, we see that the model stops and stay on a constant value on the graph 0.79 and 0.8. Upon analyizing this, we can tell that the accuracy of the model using the KNN technique is around 0.79 ~ 0.8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
